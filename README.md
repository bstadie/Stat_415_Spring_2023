# Stat 415 Spring 2023 
Stat 415 course materials



# Course Lectures 

Lecture notes can be found on the course canvas website. 


| Lecture                  |  Date | Material | Readings                
|--------------------------|-------|----------|----------------------------|
| Week 1, Tuesday          | March 28 |   No class. Watch [Lecture 0](https://www.youtube.com/watch?v=qbW3NRM-cPY) about installing python.                                          | [Scikitlearn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html) <br/> [Scikitlearn 2](https://scikit-learn.org/stable/tutorial/basic/tutorial.html) <br/> [Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) <br/> [Matplotlib](https://matplotlib.org/stable/tutorials/index.html)  |
| Week 1, Wednesday        | March 29 | Data Clearning, EDA, Class Imbalance, Imputation                                              | [EDA](https://lewtun.github.io/dslectures/lesson03_data-cleaning/) <br/> [Data Imbalance](https://imbalanced-learn.org/stable/introduction.html) <br/> [All models are wrong](https://www-sop.inria.fr/members/Ian.Jermyn/philosophy/writings/Boxonmaths.pdf)    <br/> [What's wrong with social sciences](https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/) <br/> [Future of Data Analysis](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-33/issue-1/The-Future-of-Data-Analysis/10.1214/aoms/1177704711.full) |
| Week 2, Monday           | April 3  | Information Theory   <br/> Principle Componenet Analysis (PCA)                                | [Information Theory (Chapter 2.1-2.5)](http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf) <br/> [PCA code](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html) <br/> [PCA Theory](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf) <br/> [Information Theory & Physics](https://bayes.wustl.edu/etj/articles/theory.1.pdf) |
| Week 2. Wednesday        | April 5  | Clustering, K-Means, K-Means++, DBSCAN, <br/>  Expectation Maximization (EM)                                 | [K-means overview and code](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) <br/> [K-means clustering note](https://cs229.stanford.edu/notes2020spring/cs229-notes7a.pdf) <br/> [K-means & EM Code](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html) <br/> [K-means theory](https://proceedings.neurips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf) <br/> [K-means and optimization theory](https://www.claytonthorrez.com/ml-fun/kmeans_gd/index.html) <br/> [DB Scan Wikipedia](https://en.wikipedia.org/wiki/DBSCAN) |
| Week 3, Monday           | April 10 | Recommender Systems, <br/> Popularity Filtering, <br/> Content-based Filtering, <br/> Collaborative Filtering                   | [Stanford Slides](https://web.stanford.edu/class/cs124/lec/collaborativefiltering21.pdf) <br/> [Long tutorial](http://infolab.stanford.edu/~ullman/mmds/ch9.pdf) <br/> [Starter Code for Collab Filtering](https://www.kaggle.com/code/vishorita/best-recommendation-collabarative-filtering?scriptVersionId=119356689) <br/> [Starter for content filtering](https://heartbeat.comet.ml/recommender-systems-with-python-part-i-content-based-filtering-5df4940bd831) <br/> [More content based filtering](https://www.kdnuggets.com/2020/08/content-based-recommendation-system-word-embeddings.html) <br/> [More collaborative filtering](https://realpython.com/build-recommendation-engine-collaborative-filtering/) <br/> [Matrix Factorization](https://developers.google.com/machine-learning/recommendation/collaborative/basics) <br/> [Textbook](http://pzs.dstu.dp.ua/DataMining/recom/bibl/1aggarwal_c_c_recommender_systems_the_textbook.pdf)  |
| Week 3, Wednesday        | April 12 | Linear Regression, Gradient Descent                                                           | [Boyd 9.3 (p. 466)](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf) <br/> [Gradient Descent](https://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/05-grad-descent-scribed.pdf) <br/> [SGD convergence rate](https://www.cs.ubc.ca/~schmidtm/Courses/540-W19/L11.pdf) <br/> [Andrew Ng's notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) <br/> [Linear Regression code](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_regression_in_sklearn.html) |
| Week 4, Monday           | April 17 | Classification, L1 & L2 Regularization, <br/> Cross Validation, F1-Score, <br/> Precision, AIC, Bias-Variance Tradeoff  | [Bias Variance](https://people.eecs.berkeley.edu/~jegonzal/assets/slides/linear_regression.pdf) <br/> [Andrew Ng's notes (p. 16)](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf)  |
| Week 4, Wednesday        | April 19 | Random Forests, Decision Trees                                                                | link 1  |
| Week 5, Monday           | April 24 | Deep Learning. Activation, Initialization, Advanced Optimizers                                | link 1  |
| Week 5, Wednesday        | April 26 | Convolutional Neural Nets. Batch Norm. Max Pooling                                            | link 1  |
| Week 6, Monday           | May 1    | Black Box Attacks. Interpretability. Integrated Gradients                                     | link 1  |
| Week 6, Wednesday        | May 3    | Recurrent Neural Nets, Padding <br/> Cross Validation on Time Series <br/> Autoregressive models, Machine Translaiton                             | link 1  |
| Week 7, Monday           | May 8    | Transfer Learning. Fine Tuning. MAML. Prototypical Nets                                       | [Clustering With Bregman](https://www.jmlr.org/papers/volume6/banerjee05b/banerjee05b.pdf) |
| Week 7, Wednesday        | May 10.  | Unsupervised Learning. Variational Autoencoders. <br/> Maximum Liklihood                            | link 1  |
| Week 8, Monday           | May 15.  | Generative Adversarial Nets. Diffusion Models. SD Edit                                        | link 1  |
| Week 8, Wednesday        | May 17   | How ChatGPT works. <br/> Attention, Transformers <br/>  RL, Human in the Loop Learning, Golden Labels                            | link 1  |
| Week 9, Monday           | May 22   | Pruning. Distillation. Model Growth. <br/> Quantization. Federated learning. <br/> Model drift.                         | link 1  |
| Week 9, Wednesday        | May 24   | Random Ideas. SVM, Kernel Methods. Bagging. Boosting                                          | link 1  |
| Week 10, Wednesday       | May 31   | Towards AGI. CLIP. Causality. Can wikipedia help offline RL?                                  | link 1  |



# Homeworks and Due Dates


| Project title                  | Date released | Due date                
|--------------------------------|---------------|-------------------------|
| Fraud Detection                | March 29      | April 14 (2 weeks)  |
| Reccommender Systems           | April 14      | April 28 (2 weeks)  |
| Drunk Driving Demographics     | April 28      | May 12   (2 weeks)  |
| Explainability & Animal Images | May 12        | May 31    (2 weeks) | 
| Final Exam (Coding)            | June 9        | June 9 (2 hours)    |
